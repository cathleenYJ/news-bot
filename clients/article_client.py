from newspaper import Article
from summa import summarizer
from .base_client import BaseAPIClient
from functools import lru_cache
import time

# å¸¸é‡å®šç¾©
DEFAULT_USER_AGENT = 'Mozilla/5.0'

class ArticleClient(BaseAPIClient):

    def __init__(self):
        super().__init__()
        self._cache = {}
        self._cache_timeout = 3600  # 1å°æ™‚ç·©å­˜

    def _get_cache_key(self, url):
        """ç”Ÿæˆç·©å­˜éµ"""
        return f"article_{hash(url)}"

    def _is_cache_valid(self, cache_key):
        """æª¢æŸ¥ç·©å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if cache_key in self._cache:
            timestamp, _ = self._cache[cache_key]
            return time.time() - timestamp < self._cache_timeout
        return False

    def _cleanup_cache(self):
        """æ¸…ç†éæœŸçš„ç·©å­˜é …ç›®"""
        current_time = time.time()
        expired_keys = [
            key for key, (timestamp, _) in self._cache.items()
            if current_time - timestamp > self._cache_timeout
        ]
        for key in expired_keys:
            del self._cache[key]
        if expired_keys:
            print(f"æ¸…ç†äº† {len(expired_keys)} å€‹éæœŸç·©å­˜é …ç›®")

    def process_article(self, article):
        """è™•ç†å–®ç¯‡æ–‡ç« """
        url = article['url']
        cache_key = self._get_cache_key(url)

        # å¢åŠ ç·©å­˜å¤§å°ï¼Œæ¸›å°‘æ¸…ç†é »ç‡
        if len(self._cache) > 100:  # å¢åŠ åˆ°100å€‹é …ç›®
            self._cleanup_cache()

        # æª¢æŸ¥ç·©å­˜
        if self._is_cache_valid(cache_key):
            print(f"ä½¿ç”¨ç·©å­˜çš„æ–‡ç« : {url}")
            _, cached_result = self._cache[cache_key]
            return cached_result

        try:
            response = self._make_request(url, headers={'User-Agent': DEFAULT_USER_AGENT}, allow_redirects=True, timeout=10)  # æ¸›å°‘è«‹æ±‚è¶…æ™‚
            if response:
                real_url = response.url
                art = Article(real_url)

                # è¨­ç½®æ›´çŸ­çš„è¶…æ™‚æ™‚é–“
                art.config.timeout = 8  # newspaper3kä¸‹è¼‰è¶…æ™‚
                art.download()
                art.parse()

                # æ¸…ç†ä¸éœ€è¦çš„æ•¸æ“šä»¥ç¯€çœå…§å­˜
                if hasattr(art, 'html'):
                    art.html = None

                if art.text.strip() == "":
                    summary = "ç„¡æ³•ç”Ÿæˆæ‘˜è¦"
                else:
                    # å„ªåŒ–æ‘˜è¦ç”Ÿæˆï¼šæ¸›å°‘æ–‡æœ¬é•·åº¦å’Œåƒæ•¸ä»¥æå‡é€Ÿåº¦
                    text_to_summarize = art.text[:8000]  # æ¸›å°‘åˆ°8000å­—ç¬¦
                    summary = summarizer.summarize(text_to_summarize, ratio=0.15, words=25)  # å¢åŠ ratioï¼Œæ¸›å°‘wordsä»¥åŠ å¿«è™•ç†
                    if len(summary) > 120:  # æ¸›å°‘æ‘˜è¦é•·åº¦
                        summary = summary[:120] + "..."

                news_item = f"ğŸ“° æ¨™é¡Œ: {article['title']} (ä¾†æº: {article['source']})\nğŸ”— é€£çµ: {real_url}\nğŸ“‘ æ–°èæ‘˜è¦: {summary}\n"

                # å­˜å„²åˆ°ç·©å­˜
                self._cache[cache_key] = (time.time(), news_item)

                return news_item
        except Exception as e:
            error_msg = f"Error processing {url}: {e}\n"
            # å³ä½¿å‡ºéŒ¯ä¹Ÿç·©å­˜ï¼Œé¿å…é‡è¤‡éŒ¯èª¤
            self._cache[cache_key] = (time.time(), error_msg)
            return error_msg
        finally:
            # ç¢ºä¿æ¸…ç†è³‡æº
            import gc
            gc.collect()
        return ""