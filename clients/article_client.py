from newspaper import Article
from summa import summarizer
from .base_client import BaseAPIClient
from functools import lru_cache
import time

# å¸¸é‡å®šç¾©
DEFAULT_USER_AGENT = 'Mozilla/5.0'

class ArticleClient(BaseAPIClient):

    def __init__(self):
        super().__init__()
        self._cache = {}
        self._cache_timeout = 3600  # 1å°æ™‚ç·©å­˜

    def _get_cache_key(self, url):
        """ç”Ÿæˆç·©å­˜éµ"""
        return f"article_{hash(url)}"

    def _is_cache_valid(self, cache_key):
        """æª¢æŸ¥ç·©å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if cache_key in self._cache:
            timestamp, _ = self._cache[cache_key]
            return time.time() - timestamp < self._cache_timeout
        return False

    def _cleanup_cache(self):
        """æ¸…ç†éæœŸçš„ç·©å­˜é …ç›®"""
        current_time = time.time()
        expired_keys = [
            key for key, (timestamp, _) in self._cache.items()
            if current_time - timestamp > self._cache_timeout
        ]
        for key in expired_keys:
            del self._cache[key]
        if expired_keys:
            print(f"æ¸…ç†äº† {len(expired_keys)} å€‹éæœŸç·©å­˜é …ç›®")

    def process_article(self, article):
        """è™•ç†å–®ç¯‡æ–‡ç« """
        url = article['url']
        cache_key = self._get_cache_key(url)

        # å¢åŠ ç·©å­˜å¤§å°ï¼Œæ¸›å°‘æ¸…ç†é »ç‡
        if len(self._cache) > 100:  # å¢åŠ åˆ°100å€‹é …ç›®
            self._cleanup_cache()

        # æª¢æŸ¥ç·©å­˜
        if self._is_cache_valid(cache_key):
            print(f"ä½¿ç”¨ç·©å­˜çš„æ–‡ç« : {url}")
            _, cached_result = self._cache[cache_key]
            return cached_result

        try:
            # å°ä¸åŒç¶²ç«™ä½¿ç”¨ä¸åŒçš„è¶…æ™‚ç­–ç•¥
            if 'amd.com' in url:
                request_timeout = 20  # AMDç¶²ç«™éŸ¿æ‡‰è¼ƒæ…¢ï¼Œä½¿ç”¨æ›´é•·è¶…æ™‚
                newspaper_timeout = 15
                max_retries = 1  # AMDç¶²ç«™é‡è©¦1æ¬¡
            else:
                request_timeout = 15
                newspaper_timeout = 12
                max_retries = 1

            last_exception = None

            # å¯¦ç¾é‡è©¦æ©Ÿåˆ¶
            for attempt in range(max_retries + 1):
                try:
                    response = self._make_request(url, headers={'User-Agent': DEFAULT_USER_AGENT}, allow_redirects=True, timeout=request_timeout)
                    if response:
                        real_url = response.url
                        art = Article(real_url)

                        # è¨­ç½®å°æ‡‰çš„è¶…æ™‚æ™‚é–“
                        art.config.timeout = newspaper_timeout
                        art.download()
                        art.parse()

                        # æ¸…ç†ä¸éœ€è¦çš„æ•¸æ“šä»¥ç¯€çœå…§å­˜
                        if hasattr(art, 'html'):
                            art.html = None

                        if art.text.strip() == "":
                            summary = "ç„¡æ³•ç”Ÿæˆæ‘˜è¦"
                        else:
                            # å„ªåŒ–æ‘˜è¦ç”Ÿæˆï¼šæ¸›å°‘æ–‡æœ¬é•·åº¦å’Œåƒæ•¸ä»¥æå‡é€Ÿåº¦
                            text_to_summarize = art.text[:8000]  # æ¸›å°‘åˆ°8000å­—ç¬¦
                            summary = summarizer.summarize(text_to_summarize, ratio=0.15, words=25)  # å¢åŠ ratioï¼Œæ¸›å°‘wordsä»¥åŠ å¿«è™•ç†
                            if len(summary) > 120:  # æ¸›å°‘æ‘˜è¦é•·åº¦
                                summary = summary[:120] + "..."

                        news_item = f"ğŸ“° æ¨™é¡Œ: {article['title']} (ä¾†æº: {article['source']})\nğŸ”— é€£çµ: {real_url}\nğŸ“‘ æ–°èæ‘˜è¦: {summary}\n"

                        # å­˜å„²åˆ°ç·©å­˜
                        self._cache[cache_key] = (time.time(), news_item)

                        return news_item

                except Exception as e:
                    last_exception = e
                    if attempt < max_retries:
                        print(f"è™•ç†æ–‡ç« å¤±æ•—ï¼Œé‡è©¦ {attempt + 1}/{max_retries}: {url}")
                        import time
                        time.sleep(1)  # é‡è©¦å‰ç­‰å¾…1ç§’
                    else:
                        # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†ï¼Œè¿”å›æ¨™é¡Œå’Œé€£çµ
                        print(f"è™•ç†æ–‡ç« æœ€çµ‚å¤±æ•—ï¼Œè¿”å›æ¨™é¡Œå’Œé€£çµ: {url}")
                        basic_info = f"ğŸ“° æ¨™é¡Œ: {article['title']} (ä¾†æº: {article['source']})\nğŸ”— é€£çµ: {url}\n\n"
                        # ç·©å­˜åŸºæœ¬ä¿¡æ¯ï¼Œé¿å…é‡è¤‡è™•ç†
                        self._cache[cache_key] = (time.time(), basic_info)
                        return basic_info

        except Exception as e:
            # å…¶ä»–éŒ¯èª¤ä¹Ÿè¿”å›æ¨™é¡Œå’Œé€£çµ
            print(f"è™•ç†æ–‡ç« æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ï¼Œè¿”å›æ¨™é¡Œå’Œé€£çµ: {url}")
            basic_info = f"ğŸ“° æ¨™é¡Œ: {article['title']} (ä¾†æº: {article['source']})\nğŸ”— é€£çµ: {url}\n\n"
            # ç·©å­˜åŸºæœ¬ä¿¡æ¯ï¼Œé¿å…é‡è¤‡è™•ç†
            self._cache[cache_key] = (time.time(), basic_info)
            return basic_info
        finally:
            # ç¢ºä¿æ¸…ç†è³‡æº
            import gc
            gc.collect()
        return ""