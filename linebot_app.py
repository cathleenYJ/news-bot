import requests
from newspaper import Article
from summa import summarizer
import feedparser
from flask import Flask, request, abort
from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import MessageEvent, TextMessage, TextSendMessage
from dotenv import load_dotenv
import os
from concurrent.futures import ThreadPoolExecutor
import random

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

app = Flask(__name__)

# Line Bot é…ç½®
CHANNEL_ACCESS_TOKEN = os.getenv('CHANNEL_ACCESS_TOKEN')
CHANNEL_SECRET = os.getenv('CHANNEL_SECRET')

line_bot_api = LineBotApi(CHANNEL_ACCESS_TOKEN)
handler = WebhookHandler(CHANNEL_SECRET)

def process_article(article):
    try:
        # Follow redirects for links
        response = requests.get(article['url'], headers={'User-Agent': 'Mozilla/5.0'}, allow_redirects=True)
        real_url = response.url
        art = Article(real_url)
        art.download()
        art.parse()
        if art.text.strip() == "":
            summary = "ç„¡æ³•ç”Ÿæˆæ‘˜è¦"
        else:
            # é‡å°ä¸­æ–‡å…§å®¹èª¿æ•´æ‘˜è¦åƒæ•¸
            summary = summarizer.summarize(art.text, ratio=0.1, words=30)
            # å¦‚æœæ‘˜è¦ä»ç„¶å¤ªé•·ï¼Œæ‰‹å‹•æˆªæ–·åˆ°åˆç†é•·åº¦
            if len(summary) > 150:
                summary = summary[:150] + "..."
        
        news_item = f"ğŸ“° æ¨™é¡Œ: {article['title']} (ä¾†æº: {article['source']})\nğŸ”— é€£çµ: {real_url}\nğŸ“‘ æ–°èæ‘˜è¦: {summary}\n"
        return news_item
    except Exception as e:
        return f"Error processing {article['url']}: {e}\n"

def scrape_amd_news():
    """ä½¿ç”¨ AMD API ç²å–æ–°èï¼ˆå¸¶é—œéµå­—æœå°‹ï¼‰"""
    import json
    articles = []
    try:
        api_url = 'https://xilinxcomprode2rjoqok.org.coveo.com/rest/search/v2?organizationId=xilinxcomprode2rjoqok'
        headers = {
            'accept': '*/*',
            'accept-language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'authorization': 'Bearer xx5ee91b6a-e227-4c6f-83f2-f2120ca3509e',
            'content-type': 'application/json',
            'origin': 'https://www.amd.com',
            'referer': 'https://www.amd.com/',
            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        
        # API è«‹æ±‚è³‡æ–™ - ä½¿ç”¨é—œéµå­—æœå°‹
        # Coveo API ä½¿ç”¨ OR é‚è¼¯æœå°‹å¤šå€‹é—œéµå­—
        data = {
            "locale": "zh-TW",
            "cq": "(@amd_result_type==\"Press Releases\")",
            "context": {"amd_lang": "zh-TW"},
            "q": "GPU OR AI OR é¡¯å¡ OR workstation OR é›»è…¦",  # åœ¨ API å±¤ç´šå°±ç¯©é¸é—œéµå­—
            "sortCriteria": "@amd_release_date descending",
            "numberOfResults": 10,
            "firstResult": 0
        }
        
        response = requests.post(api_url, headers=headers, json=data, timeout=15)
        
        if response.status_code == 200:
            result = response.json()
            results = result.get('results', [])
            
            for item in results[:5]:
                title = item.get('title', '').strip()
                link = item.get('clickUri', '') or item.get('uri', '')
                
                if title and link:
                    articles.append({'title': title, 'url': link, 'source': 'AMD'})
        
    except Exception as e:
        print(f"Error fetching AMD news: {e}")
    
    return articles

def scrape_nvidia_news():
    """ä½¿ç”¨ NVIDIA WordPress API ç²å–æ–°èï¼ˆå¸¶é—œéµå­—æœå°‹ï¼‰"""
    import json
    articles = []
    
    # WordPress API çš„ search åƒæ•¸åªèƒ½æœå°‹å–®ä¸€é—œéµå­—
    # æ‰€ä»¥æˆ‘å€‘æœå°‹å¤šå€‹é—œéµå­—ï¼Œåˆä½µçµæœ
    keywords = ['GPU', 'AI', 'é¡¯å¡']  # é¸æ“‡æœ€é‡è¦çš„é—œéµå­—
    all_posts = []
    seen_urls = set()
    
    try:
        api_url = 'https://blogs.nvidia.com.tw/wp-json/wp/v2/posts'
        headers = {
            'accept': 'application/json, text/plain, */*',
            'accept-language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'origin': 'https://www.nvidia.com',
            'referer': 'https://www.nvidia.com/',
            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        
        # å°æ¯å€‹é—œéµå­—é€²è¡Œæœå°‹
        for keyword in keywords:
            try:
                params = {
                    '_embed': 'true',
                    'per_page': 5,
                    'page': 1,
                    'search': keyword  # ä½¿ç”¨é—œéµå­—æœå°‹
                }
                
                response = requests.get(api_url, headers=headers, params=params, timeout=15)
                
                if response.status_code == 200:
                    posts = response.json()
                    
                    for post in posts:
                        link = post.get('link', '')
                        # å»é‡ï¼šé¿å…åŒä¸€ç¯‡æ–‡ç« è¢«å¤šå€‹é—œéµå­—æœåˆ°
                        if link and link not in seen_urls:
                            seen_urls.add(link)
                            title = post.get('title', {}).get('rendered', '').strip()
                            if title:
                                # ç§»é™¤ HTML æ¨™ç±¤
                                import re
                                title = re.sub('<.*?>', '', title)
                                all_posts.append({'title': title, 'url': link, 'source': 'NVIDIA', 'date': post.get('date', '')})
            except Exception as e:
                print(f"Error searching NVIDIA with keyword '{keyword}': {e}")
                continue
        
        # æŒ‰æ—¥æœŸæ’åºï¼Œå–æœ€æ–°çš„ 5 ç¯‡
        if all_posts:
            all_posts.sort(key=lambda x: x.get('date', ''), reverse=True)
            articles = [{'title': p['title'], 'url': p['url'], 'source': p['source']} for p in all_posts[:5]]
        
    except Exception as e:
        print(f"Error fetching NVIDIA news: {e}")
    
    return articles

def get_intel_news(keywords=None, filter_at_source=True):
    """ç²å–å¤šä¾†æºæ–°èï¼Œå¯é¸æ“‡åœ¨æŠ“å–éšæ®µé€²è¡Œé—œéµå­—ç¯©é¸"""
    if keywords is None:
        keywords = ["gpu", "é›»è…¦", "ai", "workstation", "é¡¯å¡"]
    
    # RSS feed ä¾†æº
    rss_sources = [
        {'name': 'Intel', 'url': 'https://newsroom.intel.com/zh-tw/feed/', 'type': 'rss'},
        {'name': 'Tom\'s Hardware', 'url': 'https://www.tomshardware.com/feeds/all', 'type': 'rss'},
    ]
    
    articles = []
    
    # å¾ RSS feed æŠ“å–
    for source in rss_sources:
        try:
            response = requests.get(source['url'], headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            feed = feedparser.parse(response.text)
            
            if filter_at_source and keywords:
                # ç¯©é¸åŒ…å«é—œéµå­—çš„æ–‡ç« 
                filtered_entries = []
                for entry in feed.entries:
                    title = entry.title
                    # æª¢æŸ¥æ¨™é¡Œæ˜¯å¦åŒ…å«é—œéµå­—
                    if any(keyword.lower() in title.lower() for keyword in keywords):
                        filtered_entries.append(entry)
                
                # å¾ç¯©é¸å¾Œçš„æ–‡ç« ä¸­å–æœ€å¤š5ç¯‡
                entries_to_process = filtered_entries[:5]
            else:
                # ä¸ç¯©é¸ï¼Œå–å‰5ç¯‡
                entries_to_process = feed.entries[:5]
            
            # è™•ç†é¸å®šçš„æ–‡ç« 
            for entry in entries_to_process:
                title = entry.title
                link = entry.link
                articles.append({'title': title, 'url': link, 'source': source['name']})
        except Exception as e:
            print(f"Error fetching from {source['name']}: {e}")
            continue
    
    # å¾ç¶²é çˆ¬å– AMD å’Œ NVIDIA æ–°è
    try:
        amd_articles = scrape_amd_news()
        articles.extend(amd_articles[:5])
    except Exception as e:
        print(f"Error adding AMD articles: {e}")
    
    try:
        nvidia_articles = scrape_nvidia_news()
        articles.extend(nvidia_articles[:5])
    except Exception as e:
        print(f"Error adding NVIDIA articles: {e}")
    
    # ä¸¦è¡Œè™•ç†æ–‡ç« 
    news_list = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(process_article, article) for article in articles]
        for future in futures:
            try:
                news_item = future.result(timeout=30)
                news_list.append(news_item)
            except Exception as e:
                print(f"Error processing article: {e}")
                continue

    return news_list

def get_keyword_filtered_news(news_list, keywords, target_count=5, already_filtered=False):
    """ç¯©é¸åŒ…å«é—œéµå­—çš„æ–°èï¼Œç¢ºä¿æ¯å€‹ä¾†æºè‡³å°‘ä¸€ç¯‡ï¼Œç„¶å¾Œéš¨æ©Ÿè£œå……åˆ°æŒ‡å®šæ•¸é‡
    
    Args:
        news_list: æ–°èåˆ—è¡¨
        keywords: é—œéµå­—åˆ—è¡¨
        target_count: ç›®æ¨™æ•¸é‡
        already_filtered: æ˜¯å¦å·²ç¶“åœ¨ä¾†æºå±¤ç´šé€²è¡Œéé—œéµå­—ç¯©é¸
    """
    import random

    # æŒ‰ä¾†æºåˆ†çµ„æ–°è
    source_groups = {}
    for news_item in news_list:
        if news_item.strip() and not news_item.startswith("Error"):
            # å¦‚æœå·²ç¶“åœ¨ä¾†æºå±¤ç´šç¯©é¸éï¼Œå°±ä¸éœ€è¦å†æ¬¡æª¢æŸ¥é—œéµå­—
            if already_filtered or any(keyword.lower() in news_item.lower() for keyword in keywords):
                # å¾æ–°èä¸­æå–ä¾†æº
                source_start = news_item.find("(ä¾†æº: ") + 5
                source_end = news_item.find(")", source_start)
                if source_start > 4 and source_end > source_start:
                    source = news_item[source_start:source_end]
                    if source not in source_groups:
                        source_groups[source] = []
                    source_groups[source].append(news_item.strip())

@app.route("/callback", methods=['POST'])
def callback():
    signature = request.headers['X-Line-Signature']
    body = request.get_data(as_text=True)
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        abort(400)
    return 'OK'

@handler.add(MessageEvent, message=TextMessage)
def handle_message(event):
    user_message = event.message.text.lower()
    default_keywords = ["gpu", "é›»è…¦", "ai", "workstation", "é¡¯å¡"]

    if user_message == "news":
        # ç²å–æ‰€æœ‰æ–°èä¸¦ç¯©é¸åŒ…å«é è¨­é—œéµå­—çš„5ç¯‡ï¼ˆéš¨æ©Ÿï¼Œä½†ç¢ºä¿æ¯å€‹ä¾†æºè‡³å°‘ä¸€ç¯‡ï¼‰
        # RSS ä¾†æºå·²åœ¨æŠ“å–æ™‚é€²è¡Œé—œéµå­—ç¯©é¸
        all_news = get_intel_news()
        final_news = get_keyword_filtered_news(all_news, default_keywords, target_count=5, already_filtered=True)

        if final_news:
            # ä¸€ç¯‡ä¸€ç¯‡ç™¼é€
            for news_item in final_news:
                line_bot_api.push_message(event.source.user_id, TextSendMessage(text=news_item))
        else:
            line_bot_api.reply_message(
                event.reply_token,
                TextSendMessage(text="ç›®å‰æ²’æœ‰æ‰¾åˆ°åŒ…å«é—œéµå­—çš„æ–°è")
            )

    else:
        # æª¢æŸ¥ç”¨æˆ¶è¼¸å…¥æ˜¯å¦æ˜¯ä¸€å€‹é—œéµå­—ï¼ˆå–®è©ï¼‰
        user_keyword = user_message.strip()
        if user_keyword and len(user_keyword.split()) == 1:  # ç¢ºä¿æ˜¯å–®ä¸€é—œéµå­—
            # æ ¹æ“šç”¨æˆ¶è¼¸å…¥çš„é—œéµå­—æŸ¥è©¢æ–°èï¼ˆä½¿ç”¨å‚³çµ±ç¯©é¸æ¨¡å¼ï¼Œå› ç‚ºæ˜¯è‡ªè¨‚é—œéµå­—ï¼‰
            all_news = get_intel_news(keywords=[user_keyword], filter_at_source=False)  # ä¸ä½¿ç”¨ä¾†æºå±¤ç´šç¯©é¸
            final_news = get_keyword_filtered_news(all_news, [user_keyword], target_count=5, already_filtered=False)

            if final_news:
                # ä¸€ç¯‡ä¸€ç¯‡ç™¼é€
                for news_item in final_news:
                    line_bot_api.push_message(event.source.user_id, TextSendMessage(text=news_item))
            else:
                line_bot_api.reply_message(
                    event.reply_token,
                    TextSendMessage(f"ç›®å‰æ²’æœ‰æ‰¾åˆ°åŒ…å«é—œéµå­—ã€Œ{user_keyword}ã€çš„æ–°è")
                )
        else:
            line_bot_api.reply_message(
                event.reply_token,
                TextSendMessage(text="è«‹ç™¼é€ 'news' ä¾†ç²å–æœ€æ–°åŒ…å« GPUã€é›»è…¦ã€AIã€workstationã€é¡¯å¡ é—œéµå­—çš„éš¨æ©Ÿ5ç¯‡æ–°èï¼Œæˆ–ç™¼é€ä»»ä½•å–®ä¸€é—œéµå­—ä¾†æœå°‹ç›¸é—œæ–°è")
            )

if __name__ == "__main__":
    app.run(debug=True)