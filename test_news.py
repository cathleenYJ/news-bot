#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from newspaper import Article
from summa import summarizer
import feedparser
from concurrent.futures import ThreadPoolExecutor

def process_article(article):
    try:
        print(f"æ­£åœ¨è™•ç†æ–‡ç« : {article['title'][:50]}...")
        # Follow redirects for links
        response = requests.get(article['url'], headers={'User-Agent': 'Mozilla/5.0'}, allow_redirects=True, timeout=15)
        real_url = response.url
        art = Article(real_url)

        # å¢åŠ  newspaper3k çš„è¶…æ™‚æ™‚é–“
        art.config.browser_user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        art.config.request_timeout = 15  # å¢åŠ åˆ° 15 ç§’

        art.download()
        art.parse()

        if art.text.strip() == "":
            summary = "ç„¡æ³•ç”Ÿæˆæ‘˜è¦"
        else:
            # é‡å°ä¸­æ–‡å…§å®¹èª¿æ•´æ‘˜è¦åƒæ•¸
            summary = summarizer.summarize(art.text, ratio=0.1, words=30)
            # å¦‚æœæ‘˜è¦ä»ç„¶å¤ªé•·ï¼Œæ‰‹å‹•æˆªæ–·åˆ°åˆç†é•·åº¦
            if len(summary) > 150:
                summary = summary[:150] + "..."

        news_item = f"ğŸ“° æ¨™é¡Œ: {article['title']} (ä¾†æº: {article['source']})\nğŸ”— é€£çµ: {real_url}\nğŸ“‘ æ–°èæ‘˜è¦: {summary}\n"
        return news_item
    except Exception as e:
        # è¨˜éŒ„è©³ç´°éŒ¯èª¤è¨Šæ¯
        error_msg = f"Error processing {article['url']}: {str(e)}"
        print(f"âŒ è©³ç´°éŒ¯èª¤: {error_msg}")
        print(f"   æ–‡ç« ä¾†æº: {article.get('source', 'Unknown')}")
        print(f"   æ–‡ç« æ¨™é¡Œ: {article.get('title', 'Unknown')[:50]}...")
        return error_msg

def scrape_amd_news():
    """ä½¿ç”¨ AMD API ç²å–æ–°èï¼ˆå¸¶é—œéµå­—æœå°‹ï¼‰"""
    import json
    articles = []
    try:
        api_url = 'https://xilinxcomprode2rjoqok.org.coveo.com/rest/search/v2?organizationId=xilinxcomprode2rjoqok'
        headers = {
            'accept': '*/*',
            'accept-language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'authorization': 'Bearer xx5ee91b6a-e227-4c6f-83f2-f2120ca3509e',
            'content-type': 'application/json',
            'origin': 'https://www.amd.com',
            'referer': 'https://www.amd.com/',
            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        
        # API è«‹æ±‚è³‡æ–™ - ä½¿ç”¨é—œéµå­—æœå°‹
        # Coveo API ä½¿ç”¨ OR é‚è¼¯æœå°‹å¤šå€‹é—œéµå­—
        data = {
            "locale": "zh-TW",
            "cq": "(@amd_result_type==\"Press Releases\")",
            "context": {"amd_lang": "zh-TW"},
            "q": "GPU OR AI OR é¡¯å¡ OR workstation OR é›»è…¦",  # åœ¨ API å±¤ç´šå°±ç¯©é¸é—œéµå­—
            "sortCriteria": "@amd_release_date descending",
            "numberOfResults": 10,
            "firstResult": 0
        }
        
        response = requests.post(api_url, headers=headers, json=data, timeout=15)
        
        if response.status_code == 200:
            result = response.json()
            results = result.get('results', [])
            
            for item in results[:5]:
                title = item.get('title', '').strip()
                link = item.get('clickUri', '') or item.get('uri', '')
                
                if title and link:
                    articles.append({'title': title, 'url': link, 'source': 'AMD'})
        
    except Exception as e:
        print(f"Error fetching AMD news: {e}")
    
    return articles

def scrape_nvidia_news():
    """ä½¿ç”¨ NVIDIA WordPress API ç²å–æ–°èï¼ˆå¸¶é—œéµå­—æœå°‹ï¼‰"""
    import json
    articles = []
    
    # WordPress API çš„ search åƒæ•¸åªèƒ½æœå°‹å–®ä¸€é—œéµå­—
    # æ‰€ä»¥æˆ‘å€‘æœå°‹å¤šå€‹é—œéµå­—ï¼Œåˆä½µçµæœ
    keywords = ['GPU', 'AI', 'é¡¯å¡']  # é¸æ“‡æœ€é‡è¦çš„é—œéµå­—
    all_posts = []
    seen_urls = set()
    
    try:
        api_url = 'https://blogs.nvidia.com.tw/wp-json/wp/v2/posts'
        headers = {
            'accept': 'application/json, text/plain, */*',
            'accept-language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'origin': 'https://www.nvidia.com',
            'referer': 'https://www.nvidia.com/',
            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        
        # å°æ¯å€‹é—œéµå­—é€²è¡Œæœå°‹
        for keyword in keywords:
            try:
                params = {
                    '_embed': 'true',
                    'per_page': 5,
                    'page': 1,
                    'search': keyword  # ä½¿ç”¨é—œéµå­—æœå°‹
                }
                
                response = requests.get(api_url, headers=headers, params=params, timeout=15)
                
                if response.status_code == 200:
                    posts = response.json()
                    
                    for post in posts:
                        link = post.get('link', '')
                        # å»é‡ï¼šé¿å…åŒä¸€ç¯‡æ–‡ç« è¢«å¤šå€‹é—œéµå­—æœåˆ°
                        if link and link not in seen_urls:
                            seen_urls.add(link)
                            title = post.get('title', {}).get('rendered', '').strip()
                            if title:
                                # ç§»é™¤ HTML æ¨™ç±¤
                                import re
                                title = re.sub('<.*?>', '', title)
                                all_posts.append({'title': title, 'url': link, 'source': 'NVIDIA', 'date': post.get('date', '')})
            except Exception as e:
                print(f"Error searching NVIDIA with keyword '{keyword}': {e}")
                continue
        
        # æŒ‰æ—¥æœŸæ’åºï¼Œå–æœ€æ–°çš„ 5 ç¯‡
        if all_posts:
            all_posts.sort(key=lambda x: x.get('date', ''), reverse=True)
            articles = [{'title': p['title'], 'url': p['url'], 'source': p['source']} for p in all_posts[:5]]
        
    except Exception as e:
        print(f"Error fetching NVIDIA news: {e}")
    
    return articles

def get_multi_source_news(keywords=None, filter_at_source=False):
    """ç²å–å¤šä¾†æºæ–°èï¼Œå¯é¸æ“‡åœ¨æŠ“å–éšæ®µé€²è¡Œé—œéµå­—ç¯©é¸"""
    print("é–‹å§‹æŠ“å–å¤šä¾†æºæ–°è...")
    print("=" * 70)
    if filter_at_source and keywords:
        print(f"é—œéµå­—ç¯©é¸: {', '.join(keywords)}")
        print("=" * 70)
    else:
        print("æŠ“å–æ‰€æœ‰æ–‡ç« ï¼ˆç¨å¾Œé€²è¡Œé—œéµå­—ç¯©é¸ï¼‰")
        print("=" * 70)
    
    # RSS feed ä¾†æº
    rss_sources = [
        {'name': 'Intel', 'url': 'https://newsroom.intel.com/zh-tw/feed/', 'type': 'rss'},
        {'name': 'Tom\'s Hardware', 'url': 'https://www.tomshardware.com/feeds/all', 'type': 'rss'},
    ]
    
    articles = []
    
    # å¾ RSS feed æŠ“å–
    for source in rss_sources:
        print(f"\nğŸ“¡ æŠ“å–ä¾†æº (RSS): {source['name']}")
        print(f"   URL: {source['url']}")
        try:
            response = requests.get(source['url'], headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            feed = feedparser.parse(response.text)
            print(f"   âœ… æ‰¾åˆ° {len(feed.entries)} ç¯‡æ–‡ç« ")
            
            if filter_at_source and keywords:
                # ç¯©é¸åŒ…å«é—œéµå­—çš„æ–‡ç« 
                filtered_entries = []
                for entry in feed.entries:
                    title = entry.title
                    # æª¢æŸ¥æ¨™é¡Œæ˜¯å¦åŒ…å«é—œéµå­—
                    if any(keyword.lower() in title.lower() for keyword in keywords):
                        filtered_entries.append(entry)
                
                print(f"   ğŸ” é—œéµå­—ç¯©é¸å¾Œ: {len(filtered_entries)} ç¯‡æ–‡ç« ")
                entries_to_process = filtered_entries[:5]
            else:
                # ä¸ç¯©é¸ï¼Œå–å‰5ç¯‡
                entries_to_process = feed.entries[:5]
            
            # è™•ç†é¸å®šçš„æ–‡ç« 
            for entry in entries_to_process:
                title = entry.title
                link = entry.link
                print(f"   - {title[:60]}...")
                articles.append({'title': title, 'url': link, 'source': source['name']})
        except Exception as e:
            print(f"   âŒ éŒ¯èª¤: {e}")
            continue
    
    # å¾ API ç²å– AMD æ–°è
    print(f"\nğŸ”Œ API ç²å–: AMD")
    print(f"   API: Coveo Search API")
    try:
        amd_articles = scrape_amd_news()
        if amd_articles:
            print(f"   âœ… æ‰¾åˆ° {len(amd_articles)} ç¯‡æ–‡ç« ")
            for article in amd_articles[:5]:
                print(f"   - {article['title'][:60]}...")
                articles.append(article)
        else:
            print(f"   âš ï¸  æœªæ‰¾åˆ°æ–‡ç« ")
    except Exception as e:
        print(f"   âŒ éŒ¯èª¤: {e}")
    
    # å¾ API ç²å– NVIDIA æ–°è
    print(f"\nğŸ”Œ API ç²å–: NVIDIA")
    print(f"   API: WordPress REST API")
    try:
        nvidia_articles = scrape_nvidia_news()
        if nvidia_articles:
            print(f"   âœ… æ‰¾åˆ° {len(nvidia_articles)} ç¯‡æ–‡ç« ")
            for article in nvidia_articles[:5]:
                print(f"   - {article['title'][:60]}...")
                articles.append(article)
        else:
            print(f"   âš ï¸  æœªæ‰¾åˆ°æ–‡ç« ")
    except Exception as e:
        print(f"   âŒ éŒ¯èª¤: {e}")

    if not articles:
        return "æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ç« "

    print(f"\n{'=' * 70}")
    print(f"ç¸½å…±æ”¶é›†åˆ° {len(articles)} ç¯‡æ–‡ç« ï¼Œé–‹å§‹è™•ç†...")
    print("=" * 70)
    
    # ä¸¦è¡Œè™•ç†æ–‡ç« 
    news_list = []
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = [executor.submit(process_article, article) for article in articles]
        for i, future in enumerate(futures, 1):
            try:
                news_item = future.result(timeout=30)
                if not news_item.startswith("Error"):
                    news_list.append(news_item)
                    print(f"âœ… å®Œæˆ {i}/{len(articles)}")
                else:
                    print(f"âš ï¸  è·³é {i}/{len(articles)} (è™•ç†å¤±æ•—)")
            except Exception as e:
                print(f"âŒ éŒ¯èª¤ {i}/{len(articles)}: {e}")
                continue

    return news_list

def get_keyword_filtered_news(news_list, keywords, target_count=5, already_filtered=False):
    """ç¯©é¸åŒ…å«é—œéµå­—çš„æ–°èï¼Œç¢ºä¿æ¯å€‹ä¾†æºè‡³å°‘ä¸€ç¯‡ï¼Œç„¶å¾Œéš¨æ©Ÿè£œå……åˆ°æŒ‡å®šæ•¸é‡
    
    Args:
        news_list: æ–°èåˆ—è¡¨
        keywords: é—œéµå­—åˆ—è¡¨
        target_count: ç›®æ¨™æ•¸é‡
        already_filtered: æ˜¯å¦å·²ç¶“åœ¨ä¾†æºå±¤ç´šé€²è¡Œéé—œéµå­—ç¯©é¸
    """
    import random

    # æŒ‰ä¾†æºåˆ†çµ„æ–°è
    source_groups = {}
    for news_item in news_list:
        if news_item.strip() and not news_item.startswith("Error"):
            # å¦‚æœå·²ç¶“åœ¨ä¾†æºå±¤ç´šç¯©é¸éï¼Œå°±ä¸éœ€è¦å†æ¬¡æª¢æŸ¥é—œéµå­—
            if already_filtered or any(keyword.lower() in news_item.lower() for keyword in keywords):
                # å¾æ–°èä¸­æå–ä¾†æº
                source_start = news_item.find("(ä¾†æº: ") + 5
                source_end = news_item.find(")", source_start)
                if source_start > 4 and source_end > source_start:
                    source = news_item[source_start:source_end]
                    if source not in source_groups:
                        source_groups[source] = []
                    source_groups[source].append(news_item.strip())

    # ç¢ºä¿æ¯å€‹ä¾†æºè‡³å°‘æœ‰ä¸€ç¯‡
    selected_news = []
    available_sources = list(source_groups.keys())

    # ç¬¬ä¸€è¼ªï¼šæ¯å€‹ä¾†æºé¸ä¸€ç¯‡
    for source in available_sources:
        if source_groups[source]:
            # éš¨æ©Ÿé¸ä¸€ç¯‡
            selected = random.choice(source_groups[source])
            selected_news.append(selected)
            source_groups[source].remove(selected)

    # å¦‚æœä¾†æºæ•¸é‡è¶…éç›®æ¨™æ•¸é‡ï¼Œéš¨æ©Ÿé¸æ“‡éœ€è¦çš„ä¾†æº
    if len(selected_news) > target_count:
        selected_news = random.sample(selected_news, target_count)
    elif len(selected_news) < target_count:
        # éœ€è¦è£œå……çš„æ–°èæ•¸é‡
        remaining_slots = target_count - len(selected_news)

        # æ”¶é›†æ‰€æœ‰å‰©é¤˜çš„æ–°è
        remaining_news = []
        for source in available_sources:
            remaining_news.extend(source_groups[source])

        # éš¨æ©Ÿé¸æ“‡è£œå……çš„æ–°è
        if remaining_news:
            additional_news = random.sample(remaining_news, min(remaining_slots, len(remaining_news)))
            selected_news.extend(additional_news)

    return selected_news

def test_keywords(news_list):
    print("\n" + "=" * 70)
    print("ğŸ” é—œéµå­—ç¯©é¸æ¸¬è©¦")
    print("=" * 70)
    default_keywords = ["gpu", "é›»è…¦", "ai", "workstation", "é¡¯å¡"]
    print(f"é è¨­é—œéµå­—: {', '.join(default_keywords)}\n")

    # æ¸¬è©¦é è¨­é—œéµå­—ç¯©é¸ï¼ˆæ¨¡æ“¬è¼¸å…¥ "news"ï¼‰
    print("ğŸ“ æ¸¬è©¦é è¨­é—œéµå­—ç¯©é¸ï¼ˆè¼¸å…¥ 'news'ï¼‰:")
    filtered_news_default = get_keyword_filtered_news(news_list, default_keywords, target_count=5)

    print(f"   âœ… æ‰¾åˆ° {len(filtered_news_default)} ç¯‡åŒ…å«é è¨­é—œéµå­—çš„æ–°èï¼ˆéš¨æ©Ÿ5ç¯‡ï¼Œç¢ºä¿æ¯å€‹ä¾†æºè‡³å°‘ä¸€ç¯‡ï¼‰")

    # æ¸¬è©¦è‡ªè¨‚é—œéµå­—ç¯©é¸ï¼ˆæ¨¡æ“¬è¼¸å…¥å–®ä¸€é—œéµå­—ï¼‰
    test_keywords = ["gpu", "ai", "nvidia", "intel"]
    print("\nğŸ“ æ¸¬è©¦è‡ªè¨‚é—œéµå­—ç¯©é¸ï¼ˆè¼¸å…¥å–®ä¸€é—œéµå­—ï¼‰:")

    for test_keyword in test_keywords:
        filtered_news_custom = get_keyword_filtered_news(news_list, [test_keyword], target_count=5)
        print(f"   ğŸ” é—œéµå­—ã€Œ{test_keyword}ã€: æ‰¾åˆ° {len(filtered_news_custom)} ç¯‡æ–°èï¼ˆéš¨æ©Ÿ5ç¯‡ï¼Œç¢ºä¿æ¯å€‹ä¾†æºè‡³å°‘ä¸€ç¯‡ï¼‰")

    # é¡¯ç¤ºæ‰¾åˆ°çš„æ–°è
    if filtered_news_default:
        print(f"\nâœ… é è¨­é—œéµå­—ç¯©é¸çµæœï¼ˆ{len(filtered_news_default)} ç¯‡ï¼‰:\n")
        for i, news in enumerate(filtered_news_default, 1):
            print(f"\nã€æ–°è {i}ã€‘")
            print("=" * 70)
            print(news)
    else:
        print("\nâŒ æ²’æœ‰æ‰¾åˆ°åŒ…å«é è¨­é—œéµå­—çš„æ–°è")

def test_source_level_filtering():
    """æ¸¬è©¦ä¾†æºå±¤ç´šé—œéµå­—ç¯©é¸"""
    print("\n" + "=" * 70)
    print("ğŸ” ä¾†æºå±¤ç´šé—œéµå­—ç¯©é¸æ¸¬è©¦")
    print("=" * 70)
    
    default_keywords = ["gpu", "é›»è…¦", "ai", "workstation", "é¡¯å¡"]
    print(f"é è¨­é—œéµå­—: {', '.join(default_keywords)}")
    print("ï¼ˆRSS ä¾†æºåœ¨æŠ“å–æ™‚å°±é€²è¡Œé—œéµå­—ç¯©é¸ï¼‰")
    print("=" * 70)
    
    # ä½¿ç”¨ä¾†æºå±¤ç´šç¯©é¸æŠ“å–æ–°è
    filtered_news_list = get_multi_source_news(keywords=default_keywords, filter_at_source=True)
    
    if isinstance(filtered_news_list, str):
        print(f"\nâŒ éŒ¯èª¤: {filtered_news_list}")
        return
    
    print(f"\n{'=' * 70}")
    print(f"ğŸ“Š ä¾†æºå±¤ç´šç¯©é¸çµ±è¨ˆ")
    print("=" * 70)
    print(f"æˆåŠŸè™•ç†: {len(filtered_news_list)} ç¯‡æ–‡ç« ")
    
    # çµ±è¨ˆå„ä¾†æºçš„æ–‡ç« æ•¸é‡
    source_count = {}
    for news_item in filtered_news_list:
        if news_item.strip() and not news_item.startswith("Error"):
            source_start = news_item.find("(ä¾†æº: ") + 5
            source_end = news_item.find(")", source_start)
            if source_start > 4 and source_end > source_start:
                source = news_item[source_start:source_end]
                source_count[source] = source_count.get(source, 0) + 1
    
    print("å„ä¾†æºæ–‡ç« æ•¸é‡:")
    for source, count in source_count.items():
        print(f"   {source}: {count} ç¯‡")
    
    # æ‡‰ç”¨æœ€çµ‚é¸æ“‡é‚è¼¯ï¼ˆéš¨æ©Ÿ5ç¯‡ï¼Œç¢ºä¿æ¯å€‹ä¾†æºè‡³å°‘ä¸€ç¯‡ï¼‰
    final_selection = get_keyword_filtered_news(filtered_news_list, default_keywords, target_count=5, already_filtered=True)
    
    print(f"\næœ€çµ‚é¸æ“‡: {len(final_selection)} ç¯‡æ–°è")
    if final_selection:
        print("\nâœ… æœ€çµ‚é¸æ“‡çµæœ:\n")
        for i, news in enumerate(final_selection, 1):
            print(f"\nã€æ–°è {i}ã€‘")
            print("=" * 70)
            print(news)

if __name__ == "__main__":
    print("\n" + "ğŸš€ å¤šä¾†æºæ–°èçˆ¬å–æ¸¬è©¦".center(70, "="))
    print()
    
    # æ¸¬è©¦å‚³çµ±æ¨¡å¼ï¼šæŠ“å–æ‰€æœ‰æ–‡ç« ï¼Œç„¶å¾Œé€²è¡Œé—œéµå­—ç¯©é¸
    print("ğŸ“‹ æ¸¬è©¦æ¨¡å¼ 1: å‚³çµ±é—œéµå­—ç¯©é¸ï¼ˆæŠ“å–å¾Œç¯©é¸ï¼‰")
    news_list = get_multi_source_news(filter_at_source=False)
    
    if isinstance(news_list, str):
        print(f"\nâŒ éŒ¯èª¤: {news_list}")
    else:
        print(f"\n{'=' * 70}")
        print(f"ğŸ“Š æŠ“å–çµ±è¨ˆ")
        print("=" * 70)
        print(f"æˆåŠŸè™•ç†: {len(news_list)} ç¯‡æ–‡ç« ")
        
        # æ¸¬è©¦é—œéµå­—ç¯©é¸
        test_keywords(news_list)
    
    print("\n" + "ğŸ”„ åˆ‡æ›åˆ°ä¾†æºå±¤ç´šç¯©é¸æ¸¬è©¦".center(70, "-"))
    
    # æ¸¬è©¦æ–°æ¨¡å¼ï¼šRSS ä¾†æºåœ¨æŠ“å–æ™‚å°±é€²è¡Œé—œéµå­—ç¯©é¸
    test_source_level_filtering()
    
    print("\n" + "=" * 70)
    print("âœ¨ æ¸¬è©¦å®Œæˆï¼")
    print("=" * 70)

if __name__ == "__main__":
    print("\n" + "ğŸš€ å¤šä¾†æºæ–°èçˆ¬å–æ¸¬è©¦".center(70, "="))
    print()
    
    # æ¸¬è©¦æ–°èçˆ¬å–
    news_list = get_multi_source_news()
    
    if isinstance(news_list, str):
        print(f"\nâŒ éŒ¯èª¤: {news_list}")
    else:
        print(f"\n{'=' * 70}")
        print(f"ğŸ“Š çˆ¬å–çµ±è¨ˆ")
        print("=" * 70)
        print(f"æˆåŠŸè™•ç†: {len(news_list)} ç¯‡æ–‡ç« ")
        
        # æ¸¬è©¦é—œéµå­—ç¯©é¸
        test_keywords(news_list)
    
    print("\n" + "=" * 70)
    print("âœ¨ æ¸¬è©¦å®Œæˆï¼")
    print("=" * 70)
